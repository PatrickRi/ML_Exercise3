{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "source": "import cv2\nimport numpy as np\nimport os\nimport glob\nimport pandas as pd\nimport csv\n\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import LabelEncoder",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": "imagePath\u003dr\"C:\\Users\\Patrick\\Documents\\TU\\2019S\\ML\\ML_Exercise3\\ML_Exercise3\\data\\FIDS30\"\nos.chdir(imagePath)\nfileNames \u003d glob.glob(\"*/*.jpg\")\ntargetLabels\u003d[]\nfor fileName in fileNames:\n    pathSepIndex \u003d fileName.index(\"\\\\\")\n    targetLabels.append(fileName[:pathSepIndex])\nle \u003d LabelEncoder()\nle.fit(targetLabels)\ntarget \u003d le.transform(targetLabels) ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "dico \u003d []\ndef step1():\n    fileNames \u003d glob.glob(\"*/*.jpg\")\n    for fileName in fileNames:\n        img \u003d cv2.imread(os.path.join(imagePath, fileName))\n        kp, des \u003d sift.detectAndCompute(img, None)\n        for d in des:\n            dico.append(d)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "def step2():\n    k \u003d 700\n    batch_size \u003d np.size(os.listdir(imagePath)) * 3\n    kmeans \u003d MiniBatchKMeans(n_clusters\u003dk, batch_size\u003dbatch_size, verbose\u003d1).fit(dico)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "def step3():\n    kmeans.verbose \u003d False\n    histo_list \u003d []\n    fileNames \u003d glob.glob(\"*/*.jpg\")\n    for fileName in fileNames:\n        img \u003d cv2.imread(os.path.join(imagePath, fileName))\n        kp, des \u003d sift.detectAndCompute(img, None)\n        histo \u003d np.zeros(k)\n        nkp \u003d np.size(kp)\n        for d in des:\n            idx \u003d kmeans.predict([d])\n            histo[idx] +\u003d 1/nkp # Because we need normalized histograms, I prefere to add 1/nkp directly\n        histo_list.append(histo)    ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "def step4():\n    X \u003d np.array(histo_list)\n\n    mlp \u003d MLPClassifier(verbose\u003dTrue, max_iter\u003d600)\n    mlp.fit(X, target)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "step1()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "histo_list \u003d []\nk\u003d700\nbatch_size \u003d np.size(os.listdir(imagePath)) * 3\nkmeans \u003d MiniBatchKMeans(n_clusters\u003dk, batch_size\u003dbatch_size, verbose\u003d1).fit(dico)\nfor fileName in fileNames:\n    img \u003d cv2.imread(os.path.join(imagePath, fileName))\n    #sift \u003d cv2.ORB_create()\n    sift \u003d cv2.xfeatures2d.SIFT_create()\n    keypoints, descriptors \u003d sift.detectAndCompute(img, None)\n    histo \u003d np.zeros(k)\n    nkp \u003d np.size(keypoints)\n\n    for d in descriptors:\n        idx \u003d kmeans.predict([d])\n        histo[idx] +\u003d 1/nkp # Because we need normalized histograms, I prefere to add 1/nkp directly\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": true
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}